{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3c694b-201b-4064-8cff-93b7bbcfd56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mlflow==2.21.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bd349f-4597-4433-921f-ae6617b9e8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"columbia-gr5069-main\"\n",
    "\n",
    "# Define a helper to read S3 CSV\n",
    "def read_s3_csv(key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "# Load datasets from S3\n",
    "results = read_s3_csv(\"raw/results.csv\")\n",
    "races = read_s3_csv(\"raw/races.csv\")\n",
    "drivers = read_s3_csv(\"raw/drivers.csv\")\n",
    "lap_times = read_s3_csv(\"raw/lap_times.csv\")\n",
    "pit_stops = read_s3_csv(\"raw/pit_stops.csv\")\n",
    "qualifying = read_s3_csv(\"raw/qualifying.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da03bba-4f17-4608-8dce-b1a15c5e07f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# JOIN & FEATURE CREATION SECTION\n",
    "# -------------------\n",
    "# Join: results + races + drivers\n",
    "results_merged = results.merge(races, on=\"raceId\", suffixes=(\"\", \"_race\"))\n",
    "results_merged = results_merged.merge(drivers, on=\"driverId\", suffixes=(\"\", \"_driver\"))\n",
    "\n",
    "# Feature: average lap time per driver per race\n",
    "lap_avg = lap_times.groupby(['raceId', 'driverId'])['milliseconds'].mean().reset_index()\n",
    "lap_avg.rename(columns={'milliseconds': 'avg_lap_time_ms'}, inplace=True)\n",
    "\n",
    "# Feature: number of pit stops per driver per race\n",
    "pit_count = pit_stops.groupby(['raceId', 'driverId']).size().reset_index(name='num_pit_stops')\n",
    "\n",
    "# Feature: qualifying position (lowest value if multiple attempts)\n",
    "qualifying_agg = qualifying.groupby(['raceId', 'driverId'])['position'].min().reset_index()\n",
    "qualifying_agg.rename(columns={'position': 'qualifying_position'}, inplace=True)\n",
    "\n",
    "# Merge engineered features\n",
    "results_merged = results_merged.merge(lap_avg, on=['raceId', 'driverId'], how='left')\n",
    "results_merged = results_merged.merge(pit_count, on=['raceId', 'driverId'], how='left')\n",
    "results_merged = results_merged.merge(qualifying_agg, on=['raceId', 'driverId'], how='left')\n",
    "\n",
    "# Select features for modeling\n",
    "model_data = results_merged[[\n",
    "    'raceId', 'driverId', 'grid', 'positionOrder', 'points',\n",
    "    'avg_lap_time_ms', 'num_pit_stops', 'qualifying_position']]\n",
    "\n",
    "# Drop rows with missing data\n",
    "model_data = model_data.dropna(subset=['positionOrder', 'avg_lap_time_ms', 'num_pit_stops', 'qualifying_position'])\n",
    "\n",
    "df = results_merged.dropna(subset=[\n",
    "    'positionOrder', 'avg_lap_time_ms', 'num_pit_stops', 'qualifying_position'\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(df)\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c19155c-2522-40d4-9f21-8acf9b6ddbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning up the data\n",
    "# Step 1: Replace '\\\\N' with NaN in problematic columns BEFORE split\n",
    "cols_with_N = [\n",
    "    'position', 'time', 'milliseconds', 'fastestLap', 'fastestLapTime',\n",
    "    'fastestLapSpeed', 'fp1_date', 'fp1_time', 'fp2_date', 'fp2_time',\n",
    "    'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date',\n",
    "    'sprint_time', 'number_driver'\n",
    "]\n",
    "cols_existing = [col for col in cols_with_N if col in df.columns]\n",
    "df[cols_existing] = df[cols_existing].replace('\\\\N', np.nan)\n",
    "\n",
    "# Step 2: Convert numeric-like columns\n",
    "numeric_like = ['position', 'milliseconds', 'fastestLapSpeed', 'number_driver']\n",
    "numeric_existing = [col for col in numeric_like if col in df.columns]\n",
    "for col in numeric_existing:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Step 3: Drop non-numeric columns OR encode them if needed\n",
    "df = df.select_dtypes(include=[np.number])  # Simple: drop all non-numeric cols\n",
    "\n",
    "# Step 4: Drop remaining rows with missing data or use imputer\n",
    "df = df.dropna()\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df.drop(columns=[\"positionOrder\"])\n",
    "y = df[\"positionOrder\"]\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Perform a train/test split.\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 6: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Step 7: Train model and log with MLflow\n",
    "with mlflow.start_run(run_name=\"Basic RF Experiment\") as run:\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    print(f\"  mse: {mse}\")\n",
    "  \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "  \n",
    "    runID = run.info.run_uuid\n",
    "    experimentID = run.info.experiment_id\n",
    "  \n",
    "    print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e9ab02-d83e-4268-ae46-34c50944b3ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md-sandbox\n",
    "# MAGIC ### Parameters, Metrics, and Artifacts\n",
    "# MAGIC \n",
    "# MAGIC But wait, there's more!  In the last example, you logged the run name, an evaluation metric, and your model itself as an artifact.  Now let's log parameters, multiple metrics, and other artifacts including the feature importances.\n",
    "# MAGIC \n",
    "# MAGIC First, create a function to perform this.\n",
    "# MAGIC \n",
    "# MAGIC <img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> To log artifacts, we have to save them somewhere before MLflow can log them.  This code accomplishes that by using a temporary file that it then deletes.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_rf(experimentID, run_name, params, X_train, X_test, y_train, y_test):\n",
    "  import os\n",
    "  import matplotlib.pyplot as plt\n",
    "  import mlflow.sklearn\n",
    "  import seaborn as sns\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "  import tempfile\n",
    "\n",
    "  with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n",
    "    # Create model, train it, and create predictions\n",
    "    rf = RandomForestRegressor(**params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n",
    "\n",
    "    # Log params\n",
    "    [mlflow.log_param(param, value) for param, value in params.items()]\n",
    "\n",
    "    # Create metrics\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"  mse: {}\".format(mse))\n",
    "    print(\"  mae: {}\".format(mae))\n",
    "    print(\"  R2: {}\".format(r2))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"mae\", mae)  \n",
    "    mlflow.log_metric(\"r2\", r2)  \n",
    "    \n",
    "    # Create feature importance\n",
    "    importance = pd.DataFrame(list(zip(df.columns, rf.feature_importances_)), \n",
    "                                columns=[\"Feature\", \"Importance\"]\n",
    "                              ).sort_values(\"Importance\", ascending=False)\n",
    "    \n",
    "    # Log importances using a temporary file\n",
    "    temp = tempfile.NamedTemporaryFile(prefix=\"feature-importance-\", suffix=\".csv\")\n",
    "    temp_name = temp.name\n",
    "    try:\n",
    "      importance.to_csv(temp_name, index=False)\n",
    "      mlflow.log_artifact(temp_name, \"feature-importance.csv\")\n",
    "    finally:\n",
    "      temp.close() # Delete the temp file\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.residplot(x=predictions, y=y_test, lowess=True, ax=ax)\n",
    "    plt.xlabel(\"Predicted values for Price ($)\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "\n",
    "    # Log residuals using a temporary file\n",
    "    temp = tempfile.NamedTemporaryFile(prefix=\"residuals-\", suffix=\".png\")\n",
    "    temp_name = temp.name\n",
    "    try:\n",
    "      fig.savefig(temp_name)\n",
    "      mlflow.log_artifact(temp_name, \"residuals.png\")\n",
    "    finally:\n",
    "      temp.close() # Delete the temp file\n",
    "      \n",
    "    display(fig)\n",
    "    return run.info.run_uuid\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Run with new parameters.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params1 = {\n",
    "  \"n_estimators\": 100,\n",
    "  \"max_depth\": 5,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 1: 100 Estimators, Depth 5\", params1, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params2 = {\n",
    "  \"n_estimators\": 200,\n",
    "  \"max_depth\": 5,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 2: 200 Estimators, Depth 5\", params2, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params3 = {\n",
    "  \"n_estimators\": 300,\n",
    "  \"max_depth\": 5,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 3: 300 Estimators, Depth 5\", params3, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params4 = {\n",
    "  \"n_estimators\": 100,\n",
    "  \"max_depth\": 10,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 4: 100 Estimators, Depth 10\", params4, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params5 = {\n",
    "  \"n_estimators\": 200,\n",
    "  \"max_depth\": 10,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 5: 200 Estimators, Depth 10\", params5, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params6 = {\n",
    "  \"n_estimators\": 300,\n",
    "  \"max_depth\": 10,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 6: 300 Estimators, Depth 10\", params6, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params7 = {\n",
    "  \"n_estimators\": 100,\n",
    "  \"max_depth\": 15,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 7: 100 Estimators, Depth 15\", params7, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params8 = {\n",
    "  \"n_estimators\": 200,\n",
    "  \"max_depth\": 15,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 8: 200 Estimators, Depth 15\", params8, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params9 = {\n",
    "  \"n_estimators\": 300,\n",
    "  \"max_depth\": 15,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 9: 300 Estimators, Depth 15\", params9, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "params10 = {\n",
    "  \"n_estimators\": 500,\n",
    "  \"max_depth\": 20,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Run 10: 500 Estimators, Depth 20\", params10, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d91b31a5-1620-4d06-8bc9-cc3d4542839c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The best model for this is **run 3**.\n",
    "- R² = 0.99999969, shows the highest variance explained by all the runs.\n",
    "- MSE = 0.000004887, the lowest mean squared error out of all the runs.\n",
    "- MAE = 0.0001450, also the lowest mean absolute error out of all the runs.\n",
    "\n",
    "This suggests that the model trained with n_estimators=300 and max_depth=5 provided the most accurate predictions with the best generalization performance. Therefore, Run 3 is the best model."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment 3 Jay Jun",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
